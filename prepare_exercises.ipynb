{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from acquire import get_blog_articles, get_news_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text): \n",
    "    '''This function takes in a string, lowercases it, normalized unicode characters, \n",
    "    and replaces anything that is not a letter, number, whitespace or a single quote.'''\n",
    "    # changes text to all lowercase\n",
    "    lower_case = text.lower()\n",
    "    # remove special characters, encode to ascii and recode to utf-8\n",
    "    recode = unicodedata.normalize('NFKD', low_case).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "     # Replace anything that is not a letter, number, whitespace or a single quote\n",
    "    clean_text = re.sub(r\"[^a-z0-9'\\s]\", '', recode)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    This function uses NLTK TlktokTokenizer to separate/tokenize text.\n",
    "    '''\n",
    "    # create the NLTK tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return tokenizer.tokenize(text, return_str=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    '''\n",
    "    This function applies NLTK stemming to text to remove prefix and suffixes.\n",
    "    '''\n",
    "    # Create the nltk stemmer object, then use it\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    return article_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    '''\n",
    "   This function applies NLTK lemming to text to remove prefix and suffixes.\n",
    "    '''\n",
    "    # Create the nltk lemmatize object, then use it\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    return article_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named remove_stopwords. \n",
    "- It should accept some text and return the text after removing all the stopwords.\n",
    "- This function should define two optional parameters, extra_words and exclude_words. \n",
    "    - These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function takes in text, allows for additional word be or not be excluded and removes the stopword from the text.\n",
    "    '''\n",
    "    # define initial stopwords list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    # add additional stopwords\n",
    "    for word in extra_words:\n",
    "        stopword_list.append(word)\n",
    "    # remove stopwords to exclude from stopword list\n",
    "    for word in exclude_words:\n",
    "        stopword_list.remove(word)\n",
    "    # split the string into words\n",
    "    words = text.split()\n",
    "    # filter the words\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    # print number of stopwords removed\n",
    "    # print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "    # produce string without stopwords\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    return article_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df= get_news_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Air India pilots demand vaccination on priorit...</td>\n",
       "      <td>Kiran Khatri</td>\n",
       "      <td>Indian Commercial Pilots Association (ICPA) on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>India underestimated the coronavirus: Raghuram...</td>\n",
       "      <td>Kiran Khatri</td>\n",
       "      <td>Speaking about India's second COVID-19 wave, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>South Korea's richest woman gets fortune worth...</td>\n",
       "      <td>Anmol Sharma</td>\n",
       "      <td>South Korea’s richest woman Hong Ra-hee added ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>World's biggest jeweller says it will no longe...</td>\n",
       "      <td>Kiran Khatri</td>\n",
       "      <td>Pandora, the world's biggest jeweller, has sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Will supply 11 cr doses to states, pvt hospita...</td>\n",
       "      <td>Kiran Khatri</td>\n",
       "      <td>Serum Institute of India (SII) CEO Adar Poonaw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic                                              title        author  \\\n",
       "0  business  Air India pilots demand vaccination on priorit...  Kiran Khatri   \n",
       "1  business  India underestimated the coronavirus: Raghuram...  Kiran Khatri   \n",
       "2  business  South Korea's richest woman gets fortune worth...  Anmol Sharma   \n",
       "3  business  World's biggest jeweller says it will no longe...  Kiran Khatri   \n",
       "4  business  Will supply 11 cr doses to states, pvt hospita...  Kiran Khatri   \n",
       "\n",
       "                                             content  \n",
       "0  Indian Commercial Pilots Association (ICPA) on...  \n",
       "1  Speaking about India's second COVID-19 wave, f...  \n",
       "2  South Korea’s richest woman Hong Ra-hee added ...  \n",
       "3  Pandora, the world's biggest jeweller, has sai...  \n",
       "4  Serum Institute of India (SII) CEO Adar Poonaw...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://codeup.com/codeups-data-science-career-accelerator-is-here/',\n",
    "        'https://codeup.com/data-science-myths/',\n",
    "        'https://codeup.com/data-science-vs-data-analytics-whats-the-difference/',\n",
    "        'https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/',\n",
    "        'https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/']\n",
    "codeup_df= get_blog_articles(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!   \n",
       "1                                 Data Science Myths   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                             content  \n",
       "0  The rumors are true! The time has arrived. Cod...  \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...  \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...  \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...  \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataframe, produce the following columns:\n",
    "\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function takes in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\\\n",
    "                            .apply(lemmatize)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean).apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean).apply(lemmatize)\n",
    "    \n",
    "    return df[['topic', 'title', column, 'stemmed', 'lemmatized', 'clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1643f647133b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnews_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprep_article_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'column' is not defined"
     ]
    }
   ],
   "source": [
    "news_df= prep_article_data(news_df, column, extra_words=[], exclude_words=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask yourself:\n",
    "\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
